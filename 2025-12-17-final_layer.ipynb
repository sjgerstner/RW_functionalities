{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebaf5f0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648b0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' #change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2d8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b4ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02e4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d274257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dca6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6ed857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron_choice import neuron_choice\n",
    "from entropy.entropy_intervention import make_hooks\n",
    "from argparse import Namespace\n",
    "from utils import NAME_TO_COMBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5f8ec",
   "metadata": {},
   "source": [
    "## Constants that define the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be82d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVENTION_TYPE = \"zero_ablation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6f53f",
   "metadata": {},
   "source": [
    "## Input text and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8ada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Yesterday (21 December) the Government announced a package of support for hospitality and leisure businesses that are losing trade because of the O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c710eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d94caba789041f0aab29e5bd3961731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model allenai/OLMo-7B-0424-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('allenai/OLMo-7B-0424-hf', refactor_glu=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb1fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'Yesterday', ' (', '21', ' December', ')', ' the', ' Government', ' announced', ' a', ' package', ' of', ' support', ' for', ' hospitality', ' and', ' leisure', ' businesses', ' that', ' are', ' losing', ' trade', ' because', ' of', ' the', ' O']\n"
     ]
    }
   ],
   "source": [
    "my_input_ids = model.to_tokens(prompt)\n",
    "str_tokens = model.to_str_tokens(my_input_ids)\n",
    "print(str_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcddc567",
   "metadata": {},
   "source": [
    "## List of weakening neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d78bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    work_dir='.', wcos_dir='.',\n",
    "    model='allenai/OLMo-7B-0424-hf',\n",
    "    neuron_subset_name=\"weakening\",\n",
    "    gate='-', post='+',\n",
    "    activation_location='mlp.hook_post',\n",
    "    intervention_type=INTERVENTION_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f8360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_list = neuron_choice(\n",
    "            args,\n",
    "            category_key=NAME_TO_COMBO[args.neuron_subset_name],\n",
    "            subset=243,\n",
    "            baseline=False\n",
    "        )\n",
    "#print(neuron_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd0fae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_neuron_list = [(lix, nix) for lix,nix in neuron_list if lix==model.cfg.n_layers-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9095ba7",
   "metadata": {},
   "source": [
    "## Running the model on the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91138163",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(my_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262eeef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 50304])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c1831",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c605ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_hooks(layers: list[int]|int|None=None):\n",
    "    if layers is None:\n",
    "        layers = list(range(model.cfg.n_layers))\n",
    "    elif isinstance(layers, int):\n",
    "        layers = [layers]\n",
    "    hook_list = []\n",
    "    conditioning_values = {}\n",
    "    for lix, nix in neuron_list:\n",
    "        if layers is not None and lix not in layers:\n",
    "            continue\n",
    "        conditioning_values[(lix,nix)]={}#this is possible because one list is a subset of the other\n",
    "        new_hooks = make_hooks(\n",
    "            args,\n",
    "            lix, nix,\n",
    "            conditioning_value=conditioning_values[(lix,nix)],\n",
    "            sign=(model.W_gate[lix,:,nix]@model.W_in[lix,:,nix]).item(),\n",
    "            mean_value=0.0,\n",
    "        )\n",
    "        hook_list.extend(new_hooks)\n",
    "    return hook_list, conditioning_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08166458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_clear(model=model, input_ids=my_input_ids, fwd_hooks=[], conditioning_values={}):\n",
    "    logits = model.run_with_hooks(input_ids, fwd_hooks=fwd_hooks)\n",
    "    conditioning_values = {key:{} for key in conditioning_values.keys()}\n",
    "    return logits, conditioning_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05773b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_entropy(logit_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute entropy from logits. Ignore any attention mask problems (not relevant in this notebook).\n",
    "\n",
    "    Args:\n",
    "        logit_tensor (torch.Tensor): tensor of logits, shape (batch, pos, vocab)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: tensor of entropies, shape (batch, pos)\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logit_tensor, dim=-1)\n",
    "    entropies = - torch.sum(probs * torch.log(probs + 1e-8), dim=-1)\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5f009c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_value(layers=None, input_ids=my_input_ids):\n",
    "    hook_list, conditioning_values = make_all_hooks(layers=layers)\n",
    "    logits, conditioning_values = run_and_clear(input_ids=input_ids, fwd_hooks=hook_list, conditioning_values=conditioning_values)\n",
    "    entropy = logits_to_entropy(logits)[:,-1].item()\n",
    "    del logits, hook_list\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfaf95b",
   "metadata": {},
   "source": [
    "## The crucial thing: how much of the entropy reduction is explained by the last layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e0ad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean entropy: 0.011461791582405567\n",
      "entropy when ablating all weakening neurons if gate-_post+ is fulfilled: 9.571281433105469\n",
      "entropy when ablating final layer weakening neurons if gate-_post+ is fulfilled: 0.014897964894771576\n"
     ]
    }
   ],
   "source": [
    "entropy_clean = logits_to_entropy(logits)[:,-1].item()\n",
    "del logits\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"clean entropy:\", entropy_clean)\n",
    "\n",
    "entropy_all_ablated = get_entropy_value()\n",
    "print(\"entropy when ablating all weakening neurons if gate-_post+ is fulfilled:\", entropy_all_ablated)\n",
    "\n",
    "entropy_final_ablated = get_entropy_value(31)\n",
    "print(\"entropy when ablating final layer weakening neurons if gate-_post+ is fulfilled:\", entropy_final_ablated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e796ea0",
   "metadata": {},
   "source": [
    "So it's not the final layer!!\n",
    "\n",
    "How many weakening neurons are there inside and outside of the final layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7687ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of neurons considered: 243\n",
      "number of final layer neurons considered: 58\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of neurons considered:\", len(neuron_list))\n",
    "print(\"number of final layer neurons considered:\", len(short_neuron_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c27037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "243-58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59326ea9",
   "metadata": {},
   "source": [
    "## Doing the same for one layer after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d2468ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.023790016770362854\n",
      "29 4.536865234375\n",
      "28 0.012854362837970257\n",
      "27 0.011443565599620342\n",
      "26 0.01133407186716795\n",
      "25 0.012656877748668194\n",
      "24 0.01135966181755066\n",
      "23 0.011542798951268196\n",
      "22 0.011182578280568123\n",
      "21 0.011500231921672821\n",
      "20 0.011464115232229233\n",
      "19 0.010262476280331612\n",
      "18 0.013297613710165024\n",
      "17 0.011551076546311378\n",
      "16 0.011159299872815609\n",
      "15 0.015131891705095768\n",
      "14 0.011391527950763702\n",
      "13 0.011461791582405567\n",
      "12 0.011030402965843678\n",
      "11 0.010766370221972466\n",
      "10 0.013470765203237534\n",
      "9 0.011461791582405567\n",
      "8 0.011463585309684277\n",
      "7 0.011395260691642761\n",
      "6 0.011448143981397152\n",
      "5 0.011465327814221382\n",
      "4 0.011225365102291107\n",
      "3 0.011691529303789139\n",
      "2 0.011507169343531132\n",
      "1 0.011632694862782955\n",
      "0 0.011258020997047424\n"
     ]
    }
   ],
   "source": [
    "for layer in range(30, -1, -1):\n",
    "    print(layer, get_entropy_value(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08c864",
   "metadata": {},
   "source": [
    "So layer 29 contributes a lot to that entropy increase *on its own*, while other layers *on their own* do not. So there is some cross-layer collaboration: either cross-layer superposition or some layers preparing stuff for later layers. Let's first see if we can recover (almost) the full entropy increase with layers 29 to 31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2f79a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.536865234375\n"
     ]
    }
   ],
   "source": [
    "print(get_entropy_value(29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "627c51ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11155463010072708\n"
     ]
    }
   ],
   "source": [
    "print(get_entropy_value(list(range(29,32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad2b7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8095455169677734\n"
     ]
    }
   ],
   "source": [
    "print(get_entropy_value(list(range(0,30))))#including 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edc8829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08107975125312805\n"
     ]
    }
   ],
   "source": [
    "print(get_entropy_value(list(range(29,31))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d85bdcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.571281433105469\n"
     ]
    }
   ],
   "source": [
    "print(get_entropy_value(list(range(32))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cf5a1",
   "metadata": {},
   "source": [
    "Ablating from layer n to the end, for various n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63cd315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 to 31: 0.014897964894771576\n",
      "30 to 31: 0.031053991988301277\n",
      "29 to 31: 0.11155463010072708\n",
      "28 to 31: 0.12374565750360489\n",
      "27 to 31: 0.12347183376550674\n",
      "26 to 31: 0.12399730086326599\n",
      "25 to 31: 0.14999280869960785\n",
      "24 to 31: 0.15769124031066895\n",
      "23 to 31: 0.16057711839675903\n",
      "22 to 31: 0.15990473330020905\n",
      "21 to 31: 0.16104717552661896\n",
      "20 to 31: 0.16111084818840027\n",
      "19 to 31: 0.2255602777004242\n",
      "18 to 31: 0.6157550811767578\n",
      "17 to 31: 0.7932028770446777\n",
      "16 to 31: 0.8191139698028564\n",
      "15 to 31: 0.9114950895309448\n",
      "14 to 31: 0.9104797840118408\n",
      "13 to 31: 0.9104797840118408\n",
      "12 to 31: 2.4281532764434814\n",
      "11 to 31: 6.424055099487305\n",
      "10 to 31: 9.450294494628906\n",
      "9 to 31: 9.450294494628906\n",
      "8 to 31: 9.450304985046387\n",
      "7 to 31: 9.508875846862793\n",
      "6 to 31: 9.509500503540039\n",
      "5 to 31: 9.512125015258789\n",
      "4 to 31: 9.50985050201416\n",
      "3 to 31: 9.516609191894531\n",
      "2 to 31: 9.493454933166504\n",
      "1 to 31: 9.594376564025879\n",
      "0 to 31: 9.571281433105469\n"
     ]
    }
   ],
   "source": [
    "for layer in range(31, -1, -1):\n",
    "    print(f'{layer} to 31:', get_entropy_value(list(range(layer, 32))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1289819",
   "metadata": {},
   "source": [
    "Layers 10 and maybe 11 seem crucial, but only in collaboration with (some or all) later layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda1a72",
   "metadata": {},
   "source": [
    "How about layer 10 and 29 together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27d9d750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3530426025390625\n"
     ]
    }
   ],
   "source": [
    "print(get_entropy_value(layers=[10,29]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd780e6c",
   "metadata": {},
   "source": [
    "## Finding relevant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cd565d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff = logits - logits_ablated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b12554d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 50304])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11bb30c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_logit_diff = logit_diff[0,25,:]\n",
    "relevant_logit_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c41b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([12.5329,  7.1575,  7.1229,  6.8121,  6.7228,  6.5633,  6.5550,  6.5279,\n",
      "         6.2066,  6.1819,  6.1671,  5.9645,  5.8675,  5.6778,  5.6114,  5.5637],\n",
      "       device='cuda:6', grad_fn=<TopkBackward0>),\n",
      "indices=tensor([ 6185,  6402,  7373, 10929, 34611, 28184, 18227,  8555, 43441,  2494,\n",
      "         4883, 12761, 25810, 36132,  1814, 16192], device='cuda:6'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([-3.3217, -3.2983, -3.2578, -3.1851, -3.1702, -3.1052, -3.1043, -3.0887,\n",
      "        -3.0541, -3.0457, -2.9698, -2.9301, -2.9189, -2.9149, -2.9032, -2.8691],\n",
      "       device='cuda:6', grad_fn=<TopkBackward0>),\n",
      "indices=tensor([12444, 16828,   214,  9676,  5020, 10713,  1972, 22165,   800,  3504,\n",
      "        24761, 13793, 33158,  3153,   731,  1718], device='cuda:6'))\n"
     ]
    }
   ],
   "source": [
    "top = torch.topk(relevant_logit_diff, k=16)\n",
    "bottom = torch.topk(relevant_logit_diff, k=16, largest=False)\n",
    "print(top)\n",
    "print(bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b032f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mic',\n",
       " 'MI',\n",
       " 'mi',\n",
       " ' Mic',\n",
       " 'MIC',\n",
       " 'Mic',\n",
       " 'micro',\n",
       " ' mic',\n",
       " 'yster',\n",
       " ' micro',\n",
       " 'NS',\n",
       " ' Micro',\n",
       " 'Micro',\n",
       " 'microm',\n",
       " 'mb',\n",
       " 'mn']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(top.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e51b38ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['об',\n",
       " 'ру',\n",
       " '�',\n",
       " ' illegal',\n",
       " ' majority',\n",
       " ' bulk',\n",
       " 'ances',\n",
       " ' Wayne',\n",
       " 'ative',\n",
       " 'я',\n",
       " 'houses',\n",
       " 'ع',\n",
       " '\\n\\t\\t\\n\\t',\n",
       " ' live',\n",
       " ' them',\n",
       " ' invest']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(bottom.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5cbe5d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', ' O', 'mic', 'ron']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(' Omicron')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3e1ad",
   "metadata": {},
   "source": [
    "## Ablating weakening neurons generally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d8d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.38 GiB. GPU 6 has a total capacity of 47.40 GiB of which 2.41 GiB is free. Including non-PyTorch memory, this process has 44.98 GiB memory in use. Of the allocated memory 41.08 GiB is allocated by PyTorch, and 3.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lix, nix \u001b[38;5;129;01min\u001b[39;00m neuron_list:\n\u001b[32m      7\u001b[39m     conditioning_values[(lix,nix)]={}\n\u001b[32m      8\u001b[39m     hooks_general += make_hooks(\n\u001b[32m      9\u001b[39m         args,\n\u001b[32m     10\u001b[39m         lix, nix,\n\u001b[32m     11\u001b[39m         conditioning_value=conditioning_values[(lix,nix)],\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         sign=(model.W_gate[lix,:,nix]\u001b[38;5;129m@model\u001b[39m.W_in[lix,:,nix]).item(),\n\u001b[32m     13\u001b[39m         mean_value=\u001b[32m0.0\u001b[39m,\n\u001b[32m     14\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mounts/work/sgerstner/RW_functionalities/TransformerLens/transformer_lens/HookedTransformer.py:2479\u001b[39m, in \u001b[36mHookedTransformer.W_gate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2474\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Stack the MLP gate weights across all layers.\u001b[39;00m\n\u001b[32m   2475\u001b[39m \n\u001b[32m   2476\u001b[39m \u001b[33;03mOnly works for models with gated MLPs.\u001b[39;00m\n\u001b[32m   2477\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.gated_mlp:\n\u001b[32m-> \u001b[39m\u001b[32m2479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack([block.mlp.W_gate \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks], dim=\u001b[32m0\u001b[39m)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2481\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 6 has a total capacity of 47.40 GiB of which 2.41 GiB is free. Including non-PyTorch memory, this process has 44.98 GiB memory in use. Of the allocated memory 41.08 GiB is allocated by PyTorch, and 3.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "args.gate=None\n",
    "args.post=None\n",
    "\n",
    "hooks_general = []\n",
    "conditioning_values = {}\n",
    "for layer_list, nix in neuron_list:\n",
    "    conditioning_values[(layer_list,nix)]={}\n",
    "    hooks_general += make_hooks(\n",
    "        args,\n",
    "        layer_list, nix,\n",
    "        conditioning_value=conditioning_values[(layer_list,nix)],\n",
    "        sign=(model.W_gate[layer_list,:,nix]@model.W_in[layer_list,:,nix]).item(),\n",
    "        mean_value=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711017a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9be98de5",
   "metadata": {},
   "source": [
    "## Is there a single neuron responsible for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec964a8b",
   "metadata": {},
   "source": [
    "We're looking for a weakening neuron whose $w_\\text{out}$ corresponds as closely as possible to the token 'mic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c83b2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = model.W_U[:,model.to_single_token('mic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5972e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.5654e-03, -1.9865e-02,  2.3740e-03,  2.3131e-03, -1.0920e-02,\n",
      "        -1.1843e-02, -5.0423e-03,  6.3631e-05,  6.2427e-03, -2.7632e-03,\n",
      "        -1.7804e-02, -3.3106e-03, -2.4462e-02,  1.4472e-02,  3.5042e-03,\n",
      "         1.0408e-03, -6.3770e-03,  1.5915e-02, -7.3378e-03,  4.2016e-03,\n",
      "        -2.5157e-02, -1.2029e-02, -4.9860e-04,  2.5065e-03,  1.2379e-02,\n",
      "        -1.4918e-02, -3.8517e-03,  6.9253e-04,  1.1168e-02, -1.2686e-02,\n",
      "        -2.5985e-02, -1.1120e-02,  6.2533e-02, -4.0599e-04,  4.4302e-03,\n",
      "        -7.2232e-05,  2.3728e-02, -7.4714e-03, -2.7623e-03, -9.6072e-03,\n",
      "        -7.5862e-03, -3.2695e-02, -1.2145e-03, -1.2980e-02, -6.6941e-04,\n",
      "         3.1989e-02, -3.7805e-03,  1.3651e-02, -6.6925e-03, -8.9840e-04,\n",
      "        -2.2595e-02,  7.9196e-03,  1.5796e-02, -8.6340e-03, -5.0332e-04,\n",
      "         3.4942e-04, -1.7101e-02,  8.4782e-03, -2.1459e-03, -1.4602e-02,\n",
      "         4.4703e-03,  4.5846e-03,  5.6527e-03,  1.0334e-02, -2.8266e-03,\n",
      "         1.0407e-02, -1.4816e-02,  1.0181e-03, -2.0441e-03, -7.9895e-03,\n",
      "         9.2492e-03, -3.3607e-03,  3.5227e-02,  1.2421e-02,  2.2272e-02,\n",
      "         3.5134e-02, -1.0150e-02,  3.4365e-04, -7.0647e-03, -4.3011e-03,\n",
      "         1.6703e-01, -7.7391e-03,  9.4281e-03,  4.1831e-03, -6.2609e-03,\n",
      "        -1.6635e-02, -9.6335e-03, -2.5775e-03, -7.6044e-03,  9.0736e-03,\n",
      "         1.7292e-02,  4.0892e-03, -3.4577e-03,  8.0882e-03, -2.8934e-03,\n",
      "         2.4911e-02, -2.9553e-02,  1.4572e-02, -2.5937e-03,  4.9188e-03,\n",
      "        -8.1271e-03,  1.0230e-02,  4.2897e-03,  1.8471e-03, -3.6221e-03,\n",
      "        -4.5176e-02, -2.4613e-03,  1.3402e-03, -6.4679e-03, -1.3322e-02,\n",
      "         2.5975e-02, -3.3556e-02,  4.8628e-03, -1.1124e-03, -1.3948e-02,\n",
      "         9.2180e-03,  3.0183e-03, -7.7761e-03,  6.6318e-03, -6.2454e-03,\n",
      "        -1.4202e-02,  1.9294e-02, -8.9975e-04,  8.2946e-03,  4.0775e-03,\n",
      "         5.4591e-03, -5.6742e-03,  6.2610e-03, -3.5535e-03, -8.5859e-04,\n",
      "         1.9576e-02,  3.5671e-03,  9.5594e-03, -7.1190e-03, -8.2757e-03,\n",
      "        -2.4627e-02,  5.8774e-03, -1.9354e-02,  3.2083e-02,  5.4887e-04,\n",
      "        -1.8251e-03,  3.2712e-02, -4.9627e-03, -2.8580e-03,  2.2363e-02,\n",
      "         4.2831e-03, -1.7257e-02, -3.1238e-03,  4.1873e-02, -1.2262e-03,\n",
      "        -1.1404e-02, -3.3379e-04,  5.7038e-03, -4.9630e-03, -9.3134e-04,\n",
      "         3.2711e-02,  1.4099e-02, -6.1890e-03, -1.2179e-02, -2.0915e-02,\n",
      "        -1.4448e-02,  7.3019e-03, -9.8016e-03,  2.4650e-03,  1.8396e-02,\n",
      "         8.0120e-04,  1.1479e-02, -7.7214e-03,  1.1099e-01, -3.7451e-03,\n",
      "        -3.4326e-03, -1.0480e-02, -8.3944e-03,  3.2822e-02, -3.0652e-04,\n",
      "         3.8925e-03,  6.9445e-03,  2.8455e-03,  1.1254e-03, -7.2715e-04,\n",
      "        -1.4079e-02, -9.2646e-04, -2.5498e-02, -2.1066e-02, -3.6782e-03,\n",
      "         2.9500e-03,  1.4502e-03, -2.7085e-02,  1.7520e-03,  7.5109e-03,\n",
      "        -1.1267e-03, -3.4825e-03,  1.1716e-03, -1.0791e-02, -9.7286e-03,\n",
      "         1.8739e-02,  4.7546e-03,  9.8260e-03,  4.7647e-03, -9.5568e-03,\n",
      "        -7.4834e-03, -5.2763e-03, -1.6181e-02, -4.2685e-03,  4.8152e-02,\n",
      "        -1.1209e-02,  2.4606e-02, -1.2259e-03,  1.4911e-02,  6.7225e-03,\n",
      "        -8.0408e-03,  4.7149e-03, -1.1012e-02, -4.8037e-03,  6.8458e-03,\n",
      "        -2.7527e-02,  6.1501e-03, -5.8549e-03,  4.6129e-03, -1.9232e-02,\n",
      "         6.2308e-03,  6.0249e-03,  1.0420e-02, -9.5669e-03,  3.7512e-03,\n",
      "         4.7453e-03,  1.3389e-03, -2.2484e-03,  2.3390e-03, -1.0543e-03,\n",
      "         9.8042e-03, -6.6439e-03, -1.8821e-02,  3.5331e-03,  2.1838e-03,\n",
      "         2.0130e-03, -5.0644e-03, -2.4453e-03, -1.2884e-02, -4.6698e-03,\n",
      "         7.7333e-03,  1.2097e-02, -2.0617e-02])\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for l,n in neuron_list:\n",
    "    wout = model.W_out[l,n]\n",
    "    wout /= torch.norm(wout)\n",
    "    scores.append(wout@mic)\n",
    "scores = torch.tensor(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cac6beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.1670, 0.1110, 0.0625, 0.0482, 0.0419, 0.0352, 0.0351, 0.0328, 0.0327,\n",
       "        0.0327, 0.0321, 0.0320, 0.0260, 0.0249, 0.0246, 0.0237]),\n",
       "indices=tensor([ 80, 168,  32, 204, 148,  72,  75, 173, 141, 155, 138,  45, 110,  95,\n",
       "        206,  36]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(scores, k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8042866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-0.0452, -0.0336, -0.0327, -0.0296, -0.0275, -0.0271, -0.0260, -0.0255,\n",
       "        -0.0252, -0.0246, -0.0245, -0.0226, -0.0211, -0.0209, -0.0206, -0.0199]),\n",
       "indices=tensor([105, 111,  41,  96, 215, 187,  30, 182,  20, 135,  12,  50, 183, 159,\n",
       "        242,   1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(scores, k=16, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59a874ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(31, device='cuda:0'), tensor(3498, device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_list[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd0a143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('re', tensor(0.1918, device='cuda:6'))\n",
      "('b', tensor(0.1895, device='cuda:6'))\n",
      "('c', tensor(0.1863, device='cuda:6'))\n",
      "('w', tensor(0.1831, device='cuda:6'))\n",
      "('p', tensor(0.1822, device='cuda:6'))\n",
      "('g', tensor(0.1808, device='cuda:6'))\n",
      "('to', tensor(0.1789, device='cuda:6'))\n",
      "('f', tensor(0.1776, device='cuda:6'))\n",
      "('m', tensor(0.1770, device='cuda:6'))\n",
      "('st', tensor(0.1760, device='cuda:6'))\n",
      "('sh', tensor(0.1730, device='cuda:6'))\n",
      "('de', tensor(0.1711, device='cuda:6'))\n",
      "('d', tensor(0.1711, device='cuda:6'))\n",
      "('t', tensor(0.1700, device='cuda:6'))\n",
      "('h', tensor(0.1699, device='cuda:6'))\n",
      "('se', tensor(0.1661, device='cuda:6'))\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(model.blocks[31].mlp.W_out[3498,:]@model.W_U, k=16)\n",
    "for j in range(16):\n",
    "    print((model.to_single_str_token(indices[j].item()), values[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb36c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33027, 44794, 11722, 17507, 46631, 25442, 34436, 23220, 11846, 34613,\n",
      "        46425, 25834, 46371,  5025, 15681, 22796, 31459,  8902, 36383, 11759,\n",
      "        49177, 37462, 25061, 18214,  8414, 38222, 34092, 20984,  8966, 43250,\n",
      "        48547, 35267,  1817, 25002, 11094, 23719,   204, 22263, 34918, 49288,\n",
      "        43525, 32577, 27633, 26429, 24108,  8951, 36769,  5448, 34544, 22052,\n",
      "        49540,  4933,  5594, 40541, 23993, 21516, 40685,  9317, 31069, 45143,\n",
      "        16945, 13908,  9458,  8681, 33606,  7666, 45361, 18988, 28537, 36553,\n",
      "         3530, 27726,  5330,  7637,  2332,  3959, 42244, 21798, 44665, 28791,\n",
      "          767, 43145, 14870, 10781, 29001, 45069, 48040, 27130, 45341,  3937,\n",
      "         4669, 12602, 35360,  3953, 34059,  3986, 48969,  2957, 25602, 13362,\n",
      "        47640,  1403, 12754, 20264, 25564, 31879, 29734, 16825, 36359, 44763,\n",
      "          360, 49651, 14619, 26784, 24379, 10822, 17850, 35104, 13581, 34743,\n",
      "        43165,  1346, 27405,  4292,  8312, 15292, 32876, 12042, 35807, 22554,\n",
      "          739, 15126, 11611, 44691, 43722, 48579, 15429, 31935,  7223, 21535,\n",
      "        27866,  8488, 40097, 29459,  1142, 16139, 46559, 28335,  4514, 23113,\n",
      "        47286, 24189, 14398, 36558, 26281,  4595,  6262, 32726, 47892, 47808,\n",
      "        42244, 10855, 41656, 20057,  3262, 21718, 16421, 44373,  2200, 26219,\n",
      "        28622, 47062, 42341,  3931, 19588, 11126, 13548, 17524, 15006, 27138,\n",
      "        38348, 26995, 47864, 29867, 23482, 19026, 19162, 48998, 19239,  3340,\n",
      "        27326, 30353, 20122, 45964, 39988,  3006, 15565, 10910, 13267, 45365,\n",
      "        38286, 41044, 45446, 42316,  6153, 48822, 11723, 28922, 12752, 12974,\n",
      "        42367, 10306, 42649, 31691,  6438, 43951,  6029, 36956, 14749, 29125,\n",
      "        11396,  5763,  4020, 39850, 17227, 15412, 19239, 26773, 11179, 24631,\n",
      "         4680, 43890, 47088, 16165, 17627, 16623, 33666, 31956, 41063, 30203,\n",
      "         8553,  6757, 48190])\n",
      "torch.return_types.topk(\n",
      "values=tensor([49651, 49540, 49288, 49177, 48998, 48969, 48822, 48579, 48547, 48190,\n",
      "        48040, 47892, 47864, 47808, 47640, 47286]),\n",
      "indices=tensor([111,  50,  39,  20, 187,  96, 205, 135,  30, 242,  86, 158, 182, 159,\n",
      "        100, 150]))\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "for i,(l,n) in enumerate(neuron_list):\n",
    "    wout = model.blocks[l].mlp.W_out[n,:]\n",
    "    wout /= torch.norm(wout)\n",
    "    all_scores = wout@model.W_U\n",
    "    ranks.append(torch.count_nonzero(all_scores>scores[i])-1)\n",
    "ranks = torch.tensor(ranks)\n",
    "print(ranks)\n",
    "print(torch.topk(ranks, k=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffc9c512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([ 204,  360,  739,  767, 1142, 1346, 1403, 1817, 2200, 2332, 2957, 3006,\n",
       "        3262, 3340, 3530, 3931]),\n",
       "indices=tensor([ 36, 110, 130,  80, 144, 121, 101,  32, 168,  74,  97, 195, 164, 189,\n",
       "         70, 173]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(ranks, k=16, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1609efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dbb71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d92db5",
   "metadata": {},
   "source": [
    "## Analysing model hidden states on the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bc785ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Layer 0, mid:\n",
      "([' in'], tensor(0.2816, device='cuda:6'))\n",
      "([' of'], tensor(0.2600, device='cuda:6'))\n",
      "(['\\n'], tensor(0.2556, device='cuda:6'))\n",
      "([' then'], tensor(0.2409, device='cuda:6'))\n",
      "===========================\n",
      "Layer 0, post:\n",
      "([\"'\"], tensor(0.3902, device='cuda:6'))\n",
      "(['-'], tensor(0.3567, device='cuda:6'))\n",
      "(['ops'], tensor(0.3376, device='cuda:6'))\n",
      "(['xygen'], tensor(0.3136, device='cuda:6'))\n",
      "===========================\n",
      "Layer 1, mid:\n",
      "(['xygen'], tensor(0.3256, device='cuda:6'))\n",
      "([\"'\"], tensor(0.3223, device='cuda:6'))\n",
      "(['ops'], tensor(0.3137, device='cuda:6'))\n",
      "(['ats'], tensor(0.3126, device='cuda:6'))\n",
      "===========================\n",
      "Layer 1, post:\n",
      "(['xygen'], tensor(0.3145, device='cuda:6'))\n",
      "(['ats'], tensor(0.2860, device='cuda:6'))\n",
      "(['asis'], tensor(0.2826, device='cuda:6'))\n",
      "(['ops'], tensor(0.2819, device='cuda:6'))\n",
      "===========================\n",
      "Layer 2, mid:\n",
      "(['xygen'], tensor(0.3330, device='cuda:6'))\n",
      "(['cean'], tensor(0.3086, device='cuda:6'))\n",
      "(['asis'], tensor(0.2923, device='cuda:6'))\n",
      "(['ceans'], tensor(0.2916, device='cuda:6'))\n",
      "===========================\n",
      "Layer 2, post:\n",
      "(['xygen'], tensor(0.3385, device='cuda:6'))\n",
      "(['cean'], tensor(0.2971, device='cuda:6'))\n",
      "(['BE'], tensor(0.2837, device='cuda:6'))\n",
      "(['ceans'], tensor(0.2814, device='cuda:6'))\n",
      "===========================\n",
      "Layer 3, mid:\n",
      "(['xygen'], tensor(0.3423, device='cuda:6'))\n",
      "(['cean'], tensor(0.2969, device='cuda:6'))\n",
      "(['ceans'], tensor(0.2838, device='cuda:6'))\n",
      "(['equ'], tensor(0.2796, device='cuda:6'))\n",
      "===========================\n",
      "Layer 3, post:\n",
      "(['xygen'], tensor(0.3599, device='cuda:6'))\n",
      "(['ops'], tensor(0.3491, device='cuda:6'))\n",
      "(['yster'], tensor(0.3216, device='cuda:6'))\n",
      "(['ceans'], tensor(0.3166, device='cuda:6'))\n",
      "===========================\n",
      "Layer 4, mid:\n",
      "(['ops'], tensor(0.3458, device='cuda:6'))\n",
      "(['xygen'], tensor(0.3329, device='cuda:6'))\n",
      "(['BA'], tensor(0.3206, device='cuda:6'))\n",
      "(['yster'], tensor(0.3086, device='cuda:6'))\n",
      "===========================\n",
      "Layer 4, post:\n",
      "(['xygen'], tensor(0.4415, device='cuda:6'))\n",
      "(['BA'], tensor(0.4350, device='cuda:6'))\n",
      "(['yster'], tensor(0.4014, device='cuda:6'))\n",
      "(['DE'], tensor(0.3972, device='cuda:6'))\n",
      "===========================\n",
      "Layer 5, mid:\n",
      "(['BA'], tensor(0.4521, device='cuda:6'))\n",
      "(['CC'], tensor(0.4356, device='cuda:6'))\n",
      "(['xygen'], tensor(0.4316, device='cuda:6'))\n",
      "(['CR'], tensor(0.3992, device='cuda:6'))\n",
      "===========================\n",
      "Layer 5, post:\n",
      "(['DE'], tensor(0.4178, device='cuda:6'))\n",
      "(['CC'], tensor(0.4054, device='cuda:6'))\n",
      "(['BA'], tensor(0.4011, device='cuda:6'))\n",
      "(['FA'], tensor(0.3960, device='cuda:6'))\n",
      "===========================\n",
      "Layer 6, mid:\n",
      "(['DE'], tensor(0.4475, device='cuda:6'))\n",
      "(['LE'], tensor(0.4325, device='cuda:6'))\n",
      "(['BA'], tensor(0.4286, device='cuda:6'))\n",
      "(['CR'], tensor(0.4090, device='cuda:6'))\n",
      "===========================\n",
      "Layer 6, post:\n",
      "(['LE'], tensor(0.4497, device='cuda:6'))\n",
      "(['asis'], tensor(0.4439, device='cuda:6'))\n",
      "(['xygen'], tensor(0.4436, device='cuda:6'))\n",
      "(['ahu'], tensor(0.4213, device='cuda:6'))\n",
      "===========================\n",
      "Layer 7, mid:\n",
      "(['LE'], tensor(0.4325, device='cuda:6'))\n",
      "(['asis'], tensor(0.4290, device='cuda:6'))\n",
      "(['BA'], tensor(0.4283, device='cuda:6'))\n",
      "(['xygen'], tensor(0.4164, device='cuda:6'))\n",
      "===========================\n",
      "Layer 7, post:\n",
      "(['xygen'], tensor(0.4680, device='cuda:6'))\n",
      "(['yster'], tensor(0.4599, device='cuda:6'))\n",
      "(['ahu'], tensor(0.4568, device='cuda:6'))\n",
      "(['ceans'], tensor(0.4526, device='cuda:6'))\n",
      "===========================\n",
      "Layer 8, mid:\n",
      "(['ahu'], tensor(0.4683, device='cuda:6'))\n",
      "(['ceans'], tensor(0.4673, device='cuda:6'))\n",
      "(['xygen'], tensor(0.4588, device='cuda:6'))\n",
      "(['yster'], tensor(0.4509, device='cuda:6'))\n",
      "===========================\n",
      "Layer 8, post:\n",
      "(['xygen'], tensor(0.4986, device='cuda:6'))\n",
      "(['ceans'], tensor(0.4873, device='cuda:6'))\n",
      "(['yster'], tensor(0.4845, device='cuda:6'))\n",
      "(['asis'], tensor(0.4668, device='cuda:6'))\n",
      "===========================\n",
      "Layer 9, mid:\n",
      "(['xygen'], tensor(0.4820, device='cuda:6'))\n",
      "(['ceans'], tensor(0.4807, device='cuda:6'))\n",
      "(['yster'], tensor(0.4716, device='cuda:6'))\n",
      "(['asis'], tensor(0.4485, device='cuda:6'))\n",
      "===========================\n",
      "Layer 9, post:\n",
      "([' COVID'], tensor(0.5217, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5112, device='cuda:6'))\n",
      "(['ught'], tensor(0.4808, device='cuda:6'))\n",
      "(['xygen'], tensor(0.4797, device='cuda:6'))\n",
      "===========================\n",
      "Layer 10, mid:\n",
      "([' COVID'], tensor(0.5406, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5048, device='cuda:6'))\n",
      "(['ught'], tensor(0.4788, device='cuda:6'))\n",
      "(['asis'], tensor(0.4772, device='cuda:6'))\n",
      "===========================\n",
      "Layer 10, post:\n",
      "(['ceans'], tensor(0.5270, device='cuda:6'))\n",
      "([' COVID'], tensor(0.5268, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5061, device='cuda:6'))\n",
      "(['asis'], tensor(0.4854, device='cuda:6'))\n",
      "===========================\n",
      "Layer 11, mid:\n",
      "([' COVID'], tensor(0.5312, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5232, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5124, device='cuda:6'))\n",
      "(['asis'], tensor(0.4888, device='cuda:6'))\n",
      "===========================\n",
      "Layer 11, post:\n",
      "(['ceans'], tensor(0.5157, device='cuda:6'))\n",
      "([' COVID'], tensor(0.5105, device='cuda:6'))\n",
      "(['yst'], tensor(0.5069, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5000, device='cuda:6'))\n",
      "===========================\n",
      "Layer 12, mid:\n",
      "([' COVID'], tensor(0.5346, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5197, device='cuda:6'))\n",
      "(['yst'], tensor(0.5116, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5079, device='cuda:6'))\n",
      "===========================\n",
      "Layer 12, post:\n",
      "([' COVID'], tensor(0.5646, device='cuda:6'))\n",
      "(['yst'], tensor(0.5188, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5110, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5017, device='cuda:6'))\n",
      "===========================\n",
      "Layer 13, mid:\n",
      "([' COVID'], tensor(0.5673, device='cuda:6'))\n",
      "(['yst'], tensor(0.5242, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5123, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5019, device='cuda:6'))\n",
      "===========================\n",
      "Layer 13, post:\n",
      "(['yst'], tensor(0.5571, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5215, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5150, device='cuda:6'))\n",
      "([' COVID'], tensor(0.5132, device='cuda:6'))\n",
      "===========================\n",
      "Layer 14, mid:\n",
      "([' COVID'], tensor(0.5887, device='cuda:6'))\n",
      "(['yst'], tensor(0.5605, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5259, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5252, device='cuda:6'))\n",
      "===========================\n",
      "Layer 14, post:\n",
      "(['yst'], tensor(0.5720, device='cuda:6'))\n",
      "(['yster'], tensor(0.5453, device='cuda:6'))\n",
      "([' COVID'], tensor(0.5425, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5414, device='cuda:6'))\n",
      "===========================\n",
      "Layer 15, mid:\n",
      "([' COVID'], tensor(0.5963, device='cuda:6'))\n",
      "(['yst'], tensor(0.5650, device='cuda:6'))\n",
      "(['yster'], tensor(0.5565, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5395, device='cuda:6'))\n",
      "===========================\n",
      "Layer 15, post:\n",
      "([' COVID'], tensor(0.5790, device='cuda:6'))\n",
      "(['yst'], tensor(0.5579, device='cuda:6'))\n",
      "(['yster'], tensor(0.5321, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5228, device='cuda:6'))\n",
      "===========================\n",
      "Layer 16, mid:\n",
      "([' COVID'], tensor(0.5900, device='cuda:6'))\n",
      "(['yst'], tensor(0.5712, device='cuda:6'))\n",
      "(['yster'], tensor(0.5385, device='cuda:6'))\n",
      "(['ceans'], tensor(0.5363, device='cuda:6'))\n",
      "===========================\n",
      "Layer 16, post:\n",
      "(['yst'], tensor(0.6023, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5479, device='cuda:6'))\n",
      "([' COVID'], tensor(0.5382, device='cuda:6'))\n",
      "(['yster'], tensor(0.5361, device='cuda:6'))\n",
      "===========================\n",
      "Layer 17, mid:\n",
      "(['yst'], tensor(0.6057, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5520, device='cuda:6'))\n",
      "([' COVID'], tensor(0.5418, device='cuda:6'))\n",
      "(['yster'], tensor(0.5378, device='cuda:6'))\n",
      "===========================\n",
      "Layer 17, post:\n",
      "([' COVID'], tensor(0.8083, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6614, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6458, device='cuda:6'))\n",
      "(['yst'], tensor(0.6139, device='cuda:6'))\n",
      "===========================\n",
      "Layer 18, mid:\n",
      "([' COVID'], tensor(0.8180, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6720, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6426, device='cuda:6'))\n",
      "(['yst'], tensor(0.6329, device='cuda:6'))\n",
      "===========================\n",
      "Layer 18, post:\n",
      "([' COVID'], tensor(0.7955, device='cuda:6'))\n",
      "([' virus'], tensor(0.6618, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6578, device='cuda:6'))\n",
      "(['yst'], tensor(0.6509, device='cuda:6'))\n",
      "===========================\n",
      "Layer 19, mid:\n",
      "([' COVID'], tensor(0.7899, device='cuda:6'))\n",
      "(['yst'], tensor(0.6720, device='cuda:6'))\n",
      "([' virus'], tensor(0.6517, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6511, device='cuda:6'))\n",
      "===========================\n",
      "Layer 19, post:\n",
      "([' COVID'], tensor(0.6947, device='cuda:6'))\n",
      "(['yst'], tensor(0.6250, device='cuda:6'))\n",
      "(['zone'], tensor(0.5707, device='cuda:6'))\n",
      "([' virus'], tensor(0.5634, device='cuda:6'))\n",
      "===========================\n",
      "Layer 20, mid:\n",
      "([' COVID'], tensor(0.7342, device='cuda:6'))\n",
      "(['yst'], tensor(0.6259, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6012, device='cuda:6'))\n",
      "([' Cov'], tensor(0.5869, device='cuda:6'))\n",
      "===========================\n",
      "Layer 20, post:\n",
      "([' COVID'], tensor(0.6561, device='cuda:6'))\n",
      "(['yst'], tensor(0.5995, device='cuda:6'))\n",
      "(['zone'], tensor(0.5822, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.5752, device='cuda:6'))\n",
      "===========================\n",
      "Layer 21, mid:\n",
      "([' COVID'], tensor(0.7015, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6105, device='cuda:6'))\n",
      "(['yst'], tensor(0.5933, device='cuda:6'))\n",
      "(['zone'], tensor(0.5895, device='cuda:6'))\n",
      "===========================\n",
      "Layer 21, post:\n",
      "([' COVID'], tensor(0.6500, device='cuda:6'))\n",
      "(['yst'], tensor(0.6031, device='cuda:6'))\n",
      "(['zone'], tensor(0.5806, device='cuda:6'))\n",
      "(['xygen'], tensor(0.5629, device='cuda:6'))\n",
      "===========================\n",
      "Layer 22, mid:\n",
      "([' COVID'], tensor(0.7086, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6125, device='cuda:6'))\n",
      "(['yst'], tensor(0.6019, device='cuda:6'))\n",
      "(['zone'], tensor(0.5866, device='cuda:6'))\n",
      "===========================\n",
      "Layer 22, post:\n",
      "([' COVID'], tensor(0.8053, device='cuda:6'))\n",
      "([' virus'], tensor(0.7272, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.7030, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6868, device='cuda:6'))\n",
      "===========================\n",
      "Layer 23, mid:\n",
      "([' COVID'], tensor(0.8847, device='cuda:6'))\n",
      "([' virus'], tensor(0.7852, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.7724, device='cuda:6'))\n",
      "([' Cov'], tensor(0.7438, device='cuda:6'))\n",
      "===========================\n",
      "Layer 23, post:\n",
      "([' COVID'], tensor(0.7733, device='cuda:6'))\n",
      "([' virus'], tensor(0.6929, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6906, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6373, device='cuda:6'))\n",
      "===========================\n",
      "Layer 24, mid:\n",
      "([' COVID'], tensor(0.8294, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.7238, device='cuda:6'))\n",
      "([' virus'], tensor(0.6895, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6866, device='cuda:6'))\n",
      "===========================\n",
      "Layer 24, post:\n",
      "([' COVID'], tensor(0.7282, device='cuda:6'))\n",
      "([' virus'], tensor(0.7115, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6878, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.6705, device='cuda:6'))\n",
      "===========================\n",
      "Layer 25, mid:\n",
      "([' Christmas'], tensor(1.0315, device='cuda:6'))\n",
      "([' winter'], tensor(0.7812, device='cuda:6'))\n",
      "([' COVID'], tensor(0.7426, device='cuda:6'))\n",
      "([' Cov'], tensor(0.6984, device='cuda:6'))\n",
      "===========================\n",
      "Layer 25, post:\n",
      "([' Christmas'], tensor(0.8849, device='cuda:6'))\n",
      "([' COVID'], tensor(0.7236, device='cuda:6'))\n",
      "([' Cov'], tensor(0.7039, device='cuda:6'))\n",
      "(['zone'], tensor(0.6824, device='cuda:6'))\n",
      "===========================\n",
      "Layer 26, mid:\n",
      "([' Christmas'], tensor(0.9251, device='cuda:6'))\n",
      "([' COVID'], tensor(0.8307, device='cuda:6'))\n",
      "([' Cov'], tensor(0.7684, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.7471, device='cuda:6'))\n",
      "===========================\n",
      "Layer 26, post:\n",
      "([' Christmas'], tensor(0.8168, device='cuda:6'))\n",
      "([' COVID'], tensor(0.7416, device='cuda:6'))\n",
      "(['zone'], tensor(0.7384, device='cuda:6'))\n",
      "(['xygen'], tensor(0.7311, device='cuda:6'))\n",
      "===========================\n",
      "Layer 27, mid:\n",
      "([' Christmas'], tensor(0.8687, device='cuda:6'))\n",
      "([' COVID'], tensor(0.7853, device='cuda:6'))\n",
      "([' Cov'], tensor(0.7589, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.7405, device='cuda:6'))\n",
      "===========================\n",
      "Layer 27, post:\n",
      "([' COVID'], tensor(0.7874, device='cuda:6'))\n",
      "(['yster'], tensor(0.7553, device='cuda:6'))\n",
      "(['xygen'], tensor(0.7518, device='cuda:6'))\n",
      "(['zone'], tensor(0.7373, device='cuda:6'))\n",
      "===========================\n",
      "Layer 28, mid:\n",
      "(['yster'], tensor(0.7633, device='cuda:6'))\n",
      "(['xygen'], tensor(0.7624, device='cuda:6'))\n",
      "([' COVID'], tensor(0.7456, device='cuda:6'))\n",
      "(['asis'], tensor(0.7230, device='cuda:6'))\n",
      "===========================\n",
      "Layer 28, post:\n",
      "([' COVID'], tensor(1.1920, device='cuda:6'))\n",
      "([' Cov'], tensor(1.0926, device='cuda:6'))\n",
      "([' coronavirus'], tensor(0.9585, device='cuda:6'))\n",
      "(['xygen'], tensor(0.9219, device='cuda:6'))\n",
      "===========================\n",
      "Layer 29, mid:\n",
      "([' COVID'], tensor(1.2513, device='cuda:6'))\n",
      "([' Cov'], tensor(1.1395, device='cuda:6'))\n",
      "([' coronavirus'], tensor(1.0510, device='cuda:6'))\n",
      "([' pandemic'], tensor(0.9720, device='cuda:6'))\n",
      "===========================\n",
      "Layer 29, post:\n",
      "(['mic'], tensor(1.2195, device='cuda:6'))\n",
      "(['-'], tensor(1.1972, device='cuda:6'))\n",
      "(['.'], tensor(1.0594, device='cuda:6'))\n",
      "(['yster'], tensor(0.9831, device='cuda:6'))\n",
      "===========================\n",
      "Layer 30, mid:\n",
      "(['mic'], tensor(1.2065, device='cuda:6'))\n",
      "(['yster'], tensor(1.0270, device='cuda:6'))\n",
      "(['xygen'], tensor(1.0268, device='cuda:6'))\n",
      "(['culus'], tensor(0.9159, device='cuda:6'))\n",
      "===========================\n",
      "Layer 30, post:\n",
      "(['-'], tensor(1.5444, device='cuda:6'))\n",
      "(['mic'], tensor(1.5315, device='cuda:6'))\n",
      "(['.'], tensor(1.4397, device='cuda:6'))\n",
      "(['2'], tensor(1.2832, device='cuda:6'))\n",
      "===========================\n",
      "Layer 31, mid:\n",
      "(['mic'], tensor(1.6321, device='cuda:6'))\n",
      "(['-'], tensor(1.5755, device='cuda:6'))\n",
      "(['.'], tensor(1.5584, device='cuda:6'))\n",
      "(['2'], tensor(1.3108, device='cuda:6'))\n",
      "===========================\n",
      "Layer 31, post:\n",
      "(['mic'], tensor(4.8025, device='cuda:6'))\n",
      "(['MI'], tensor(2.6920, device='cuda:6'))\n",
      "(['mi'], tensor(2.6101, device='cuda:6'))\n",
      "([' mic'], tensor(2.6015, device='cuda:6'))\n"
     ]
    }
   ],
   "source": [
    "for layer in range(32):\n",
    "    for sublayer in ['mid', 'post']:\n",
    "        print('===========================')\n",
    "        print(f'Layer {layer}, {sublayer}:')\n",
    "        top_token_vi = torch.topk(cache[f'blocks.{layer}.hook_resid_{sublayer}'][0,25]@model.W_U, k=4)\n",
    "        for j in range(4):\n",
    "            print((model.to_str_tokens(top_token_vi.indices[j]), top_token_vi.values[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfd1f67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, mid: -0.009818505495786667\n",
      "Layer 0, post: 0.1756778061389923\n",
      "Layer 1, mid: 0.1873852014541626\n",
      "Layer 1, post: 0.16365423798561096\n",
      "Layer 2, mid: 0.18060725927352905\n",
      "Layer 2, post: 0.17816656827926636\n",
      "Layer 3, mid: 0.16314618289470673\n",
      "Layer 3, post: 0.19512557983398438\n",
      "Layer 4, mid: 0.18644800782203674\n",
      "Layer 4, post: 0.20833870768547058\n",
      "Layer 5, mid: 0.16482438147068024\n",
      "Layer 5, post: 0.15313085913658142\n",
      "Layer 6, mid: 0.14475101232528687\n",
      "Layer 6, post: 0.22487929463386536\n",
      "Layer 7, mid: 0.22021272778511047\n",
      "Layer 7, post: 0.2116979956626892\n",
      "Layer 8, mid: 0.19317162036895752\n",
      "Layer 8, post: 0.2249262034893036\n",
      "Layer 9, mid: 0.2260773777961731\n",
      "Layer 9, post: 0.187677800655365\n",
      "Layer 10, mid: 0.1936216801404953\n",
      "Layer 10, post: 0.2052566558122635\n",
      "Layer 11, mid: 0.22408485412597656\n",
      "Layer 11, post: 0.19123020768165588\n",
      "Layer 12, mid: 0.21115386486053467\n",
      "Layer 12, post: 0.23132723569869995\n",
      "Layer 13, mid: 0.2442898154258728\n",
      "Layer 13, post: 0.25902628898620605\n",
      "Layer 14, mid: 0.27178335189819336\n",
      "Layer 14, post: 0.27256208658218384\n",
      "Layer 15, mid: 0.2834193706512451\n",
      "Layer 15, post: 0.28171712160110474\n",
      "Layer 16, mid: 0.29786670207977295\n",
      "Layer 16, post: 0.32215315103530884\n",
      "Layer 17, mid: 0.3280490040779114\n",
      "Layer 17, post: 0.331258624792099\n",
      "Layer 18, mid: 0.35089704394340515\n",
      "Layer 18, post: 0.3472024202346802\n",
      "Layer 19, mid: 0.3690873384475708\n",
      "Layer 19, post: 0.3712890148162842\n",
      "Layer 20, mid: 0.3847208619117737\n",
      "Layer 20, post: 0.4008021652698517\n",
      "Layer 21, mid: 0.4051680266857147\n",
      "Layer 21, post: 0.4263611137866974\n",
      "Layer 22, mid: 0.42865175008773804\n",
      "Layer 22, post: 0.42717283964157104\n",
      "Layer 23, mid: 0.431043803691864\n",
      "Layer 23, post: 0.44019821286201477\n",
      "Layer 24, mid: 0.44355693459510803\n",
      "Layer 24, post: 0.4863126277923584\n",
      "Layer 25, mid: 0.48850756883621216\n",
      "Layer 25, post: 0.5271747708320618\n",
      "Layer 26, mid: 0.5428463220596313\n",
      "Layer 26, post: 0.5952954888343811\n",
      "Layer 27, mid: 0.6060255169868469\n",
      "Layer 27, post: 0.639245331287384\n",
      "Layer 28, mid: 0.6510517597198486\n",
      "Layer 28, post: 0.8819511532783508\n",
      "Layer 29, mid: 0.8610501885414124\n",
      "Layer 29, post: 1.2195000648498535\n",
      "Layer 30, mid: 1.2065311670303345\n",
      "Layer 30, post: 1.5314865112304688\n",
      "Layer 31, mid: 1.6320769786834717\n",
      "Layer 31, post: 4.80251932144165\n"
     ]
    }
   ],
   "source": [
    "for layer in range(32):\n",
    "    for sublayer in ['mid', 'post']:\n",
    "        print(f'Layer {layer}, {sublayer}: {cache[f'blocks.{layer}.hook_resid_{sublayer}'][0,25]@mic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a2d96",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "io2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
